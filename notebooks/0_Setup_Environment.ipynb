{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: left;\">\n",
    "    <a href=\"https://sites.google.com/corp/google.com/genai-solutions/home?authuser=0\">\n",
    "        <img src=\"https://storage.googleapis.com/miscfilespublic/Linkedin%20Banner%20%E2%80%93%202.png\" style=\"margin-right\">\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRyGcAepAPJ5"
   },
   "source": [
    "\n",
    "<h1 align=\"center\">Open Data QnA - Chat with your SQL Database</h1> \n",
    "\n",
    "---\n",
    "\n",
    "This notebook first walks through the Vector Store Setup needed for running the Open Data QnA application. \n",
    "\n",
    "Currently supported Source DBs are: \n",
    "- PostgreSQL on Google Cloud SQL \n",
    "- BigQuery\n",
    "\n",
    "Furthermore, the following vector stores are supported \n",
    "- pgvector on PostgreSQL \n",
    "- BigQuery vector\n",
    "\n",
    "\n",
    "The setup part covers the following steps: \n",
    "> 1. Configuration: Intial GCP project, IAM permissions, Environment  and Databases setup including logging on Bigquery for analytics\n",
    "\n",
    "> 2. Creation of Table, Column and Known Good Query Embeddings in the Vector Store  for Retreival Augmented Generation(RAG)\n",
    "\n",
    "\n",
    "Afterwards, you will be able to run the Open Data QnA Pipeline to generate SQL queries and answer questions over your data source. \n",
    "\n",
    "The pipeline run covers the following steps: \n",
    "\n",
    "> 1. Take user question and generate sql in the dialect corresponding to data source\n",
    "\n",
    "> 2. Execute the sql query and retreive the data\n",
    "\n",
    "> 3. Generate natural language respose and charts to display\n",
    "\n",
    "> 4. Clean Up resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsWGZW_fUJjN"
   },
   "source": [
    "### üìí Using this interactive notebook\n",
    "\n",
    "If you have not used this IDE with jupyter notebooks it will ask for installing Python + Jupyter extensions. Please go ahead install them\n",
    "\n",
    "Click the **run** icons ‚ñ∂Ô∏è  of each cell within this notebook.\n",
    "\n",
    "> üí° Alternatively, you can run the currently selected cell with `Ctrl + Enter` (or `‚åò + Enter` on a Mac).\n",
    "\n",
    "> ‚ö†Ô∏è **To avoid any errors**, wait for each section to finish in their order before clicking the next ‚Äúrun‚Äù icon.\n",
    "\n",
    "This sample must be connected to a **Google Cloud project**, but nothing else is needed other than your Google Cloud project.\n",
    "\n",
    "You can use an existing project. Alternatively, you can create a new Cloud project [with cloud credits for free.](https://cloud.google.com/free/docs/gcp-free-tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RicDCkdI-hmp"
   },
   "source": [
    "# üöß **0. Prerequisites**\n",
    "\n",
    "Make sure that Google Cloud CLI is installed before moving to the next cell! You can refer to the link below for guidance\n",
    "\n",
    "Installation Guide: https://cloud.google.com/sdk/docs/install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **0.1. Setup Poetry Environment and Setup GCP Project** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª **Install Code Dependencies (Create and setup venv)**\n",
    "Install the dependencies by runnign the poetry commands below \n",
    "\n",
    "Note: Below command runs with default Python Kernel and we will change that to Kernel from venv after this execution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install poetry\n",
    "#! pip uninstall poetry -y\n",
    "! pip install poetry --quiet\n",
    "\n",
    "#Run the poetry commands below to set up the environment\n",
    "!poetry lock #resolve dependecies (also auto create poetry venv if not exists)\n",
    "!poetry install --quiet #installs dependencies\n",
    "!poetry env info #Displays the env just created and the path to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå **Important Step: Activate your virtual environment and authenticate with Google Cloud CLI**\n",
    "\n",
    "#### **All commands in this cell to be run on Terminal**\n",
    "\n",
    "\n",
    "Chose the relevant instructions based on where you are running this notebook\n",
    "\n",
    "\n",
    "**For IDEs like Cloud Shell Editor, VS Code**\n",
    "\n",
    "Once the venv created either in the local directory or in the cache directory. Open the terminal on the same machine where your notebooks are running and start running the below commands.\n",
    "\n",
    "\n",
    "~~~bash\n",
    "poetry shell #this command should activate your venv and you should see it enters into the venv\n",
    "\n",
    "##inside the activated venv shell []\n",
    "\n",
    "gcloud auth login\n",
    "gcloud auth application-default login\n",
    "gcloud services enable serviceusage.googleapis.com cloudresourcemanager.googleapis.com --project <<Project_Id>>\n",
    "gcloud auth application-default set-quota-project <<Project_Id>>\n",
    "~~~\n",
    "\n",
    "For IDEs adding Juypter Extensions will automatically give you option to change the kernel. If not, manually select the python interpreter in your IDE (The exact is shown in the above cell. Path would look like e.g. /home/admin_/Talk2Data/.venv/bin/python or ~cache/user/Talk2Data/.venv/bin/python)\n",
    "\n",
    "\n",
    "**For Jupyter Lab or Jupyter Environments on Workbench etc**\n",
    "\n",
    "\n",
    "```bash\n",
    "poetry shell #this command should activate your venv and you should see it enters into the venv\n",
    "\n",
    "##inside the activated venv shell []\n",
    "\n",
    "#If you are running on Worbench instance where the service account used has required permissions to run this solution you can skip the below gcloud auth commands and get to next kernel creation section\n",
    "gcloud auth login\n",
    "gcloud auth application-default login\n",
    "gcloud services enable serviceusage.googleapis.com cloudresourcemanager.googleapis.com --project <<Project_Id>>\n",
    "gcloud auth application-default set-quota-project <<Project_Id>>\n",
    "\n",
    "```\n",
    "\n",
    "Create Kernel for with the envrionment created\n",
    "\n",
    "```bash\n",
    "\n",
    "pip install jupyter\n",
    "\n",
    "ipython kernel install --name \"openqna-venv\" --user \n",
    "\n",
    "```\n",
    "\n",
    "Restart your kernel or close the exsiting notebook and open again, you should now see the \"openqna-venv\" in the kernel drop down\n",
    "\n",
    "**What did we do here?**\n",
    "\n",
    "* Created Application Default Credentials to use for the code\n",
    "* Added venv to kernel to select for runningt the notebooks (For standalone Jupyter setups like Workbench etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Python Module Path to Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "format_string = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter(format_string)\n",
    "fhandler.setFormatter(formatter)\n",
    "#logger.addHandler(fhandler)\n",
    "logging.basicConfig(format=format_string,\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4W6FPnrYEE8"
   },
   "source": [
    "### üîó **Connect Your Google Cloud Project**\n",
    "Time to connect your Google Cloud Project to this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Please fill in the value below with your GCP project ID and then run the cell.\n",
    "PROJECT_ID = \"uk-bh-experiments-argolis\"\n",
    "\n",
    "# Quick input validations.\n",
    "assert PROJECT_ID, \"‚ö†Ô∏è Please provide your Google Cloud Project ID\"\n",
    "\n",
    "# Configure gcloud.\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "print(f'Project has been set to {PROJECT_ID}')\n",
    "\n",
    "os.environ['GOOGLE_CLOUD_QUOTA_PROJECT']=PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è **Enable Required API Services in the GCP Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable all the required APIs for the Open Data QnA solution\n",
    "\n",
    "!gcloud services enable \\\n",
    "  cloudapis.googleapis.com \\\n",
    "  compute.googleapis.com \\\n",
    "  iam.googleapis.com \\\n",
    "  run.googleapis.com \\\n",
    "  sqladmin.googleapis.com \\\n",
    "  aiplatform.googleapis.com \\\n",
    "  bigquery.googleapis.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Vector Store Setup** (Run once)\n",
    "---\n",
    "\n",
    "This section walks through the Vector Store Setup needed for running the Open Data QnA application. \n",
    "\n",
    "It covers the following steps: \n",
    "> 1. Configuration: Environment and Databases setup including logging on Bigquery for analytics\n",
    "\n",
    "> 2. Creation of Table, Column and Known Good Query Embeddings in the Vector Store  for Retreival Augmented Generation(RAG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà **1.1 Set Up your Data Source and Vector Store**\n",
    "\n",
    "This section assumes that a datasource is already set up in your GCP project. If a datasource has not been set up, use the notebooks below to copy a public data set from BigQuery to Cloud SQL or BigQuery on your GCP project\n",
    "\n",
    "\n",
    "Enabled Data Sources:\n",
    "* PostgreSQL on Google Cloud SQL (Copy Sample Data: [0_CopyDataToCloudSqlPG.ipynb](0_CopyDataToCloudSqlPG.ipynb))\n",
    "* BigQuery (Copy Sample Data: [0_CopyDataToBigQuery.ipynb](0_CopyDataToBigQuery.ipynb))\n",
    "\n",
    "Enabled Vector Stores:\n",
    "* pgvector on PostgreSQL \n",
    "* BigQuery vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î **Choose Data Source and Vector Store**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the parameters and configuration settings below. \n",
    "These are the parameters for connecting to the source databases and setting configurations for the vector store tables to be created. \n",
    "Additionally, you can specify whether you have and want to use known-good-queries for the pipeline run and whether you want to enable logging.\n",
    "\n",
    "**Known good queries:** if you have known working user question <-> SQL query pairs, you can put them into the file `scripts/known_good_sql.csv`. This will be used as a caching layer and for in-context learning: If an exact match of the user question is found in the vector store, the pipeline will skip SQL Generation and output the cached SQL query. If the similarity score is between 90-100%, the known good queries will be used as few-shot examples by the SQL Generator Agent. \n",
    "\n",
    "**Logging:** you can enable logging. If enabled, a dataset is created in Big Query in your project, which will store the logging table and save information from the pipeline run in the logging table. This is especially helpful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CONFIG]\n",
    "embedding_model = 'vertex' # Options: 'vertex' or 'vertex-lang'\n",
    "vector_embedding_model = \"text-embedding-004\"\n",
    "description_model = 'gemini-1.5-pro-001' # 'gemini-1.5-pro-001', 'gemini-1.5-pro', 'text-bison-32k'\n",
    "data_source = 'bigquery' #  Options: 'bigquery' and 'cloudsql-pg' \n",
    "vector_store = 'bigquery-vector' # Options: 'bigquery-vector', 'cloudsql-pgvector'\n",
    "logging = True # True or False \n",
    "kgq_examples = True # True or False \n",
    "\n",
    "#[GCP]\n",
    "project_id = PROJECT_ID\n",
    "\n",
    "#[PGCLOUDSQL]\n",
    "# If you want to use PG as source, fill out the values below\n",
    "pg_region = ''\n",
    "pg_instance = ''\n",
    "pg_database = ''\n",
    "pg_user = ''\n",
    "pg_password = ''\n",
    "pg_schema = ''\n",
    "\n",
    "#[BIGQUERY]\n",
    "# If you want to use BQ as source, fill out the values below\n",
    "bq_dataset_region = 'us-central1'\n",
    "bq_dataset_name = 'breedr'\n",
    "\n",
    "# Name for the BQ dataset created for bigquery-vector and/or logging\n",
    "bq_opendataqna_dataset_name = 'opendataqna'\n",
    "bq_log_table_name = 'audit_log_table' \n",
    "bq_table_list = None # either None or a list of table names in format ['reviews', 'ratings']\n",
    "\n",
    "#Decode Region and Userdatabase based on source\n",
    "if data_source == 'bigquery': dataset_region = bq_dataset_region; user_database=bq_dataset_name \n",
    "elif data_source == 'cloudsql-pg': dataset_region = pg_region; user_database=pg_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick input verifications below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input verification - Source\n",
    "assert data_source in {'bigquery', 'cloudsql-pg'}, \"‚ö†Ô∏è Invalid DATA_SOURCE. Must be 'bigquery' or 'cloudsql-pg'\"\n",
    "\n",
    "# Input verification - Vector Store\n",
    "assert vector_store in {'bigquery-vector', 'cloudsql-pgvector'}, \"‚ö†Ô∏è Invalid VECTOR_STORE. Must be 'bigquery-vector' or 'cloudsql-pgvector'\"\n",
    "\n",
    "if logging: \n",
    "    assert bq_log_table_name, \"‚ö†Ô∏è Please provide a name for your log table if you want to use logging\"\n",
    "\n",
    "if data_source == 'bigquery':\n",
    "    assert bq_dataset_region, \"‚ö†Ô∏è Please provide the Data Set Region\"\n",
    "    assert bq_dataset_name, \"‚ö†Ô∏è Please provide the name of the dataset on Bigquery\"\n",
    "\n",
    "elif data_source == 'cloudsql-pg':\n",
    "    assert pg_region, \"‚ö†Ô∏è Please provide Region of the Cloud SQL Instance\"\n",
    "    assert pg_instance, \"‚ö†Ô∏è Please provide the name of the Cloud SQL Instance\"\n",
    "    assert pg_database, \"‚ö†Ô∏è Please provide the name of the PostgreSQL Database on the Cloud SQL Instance\"\n",
    "    assert pg_user, \"‚ö†Ô∏è Please provide a username for the Cloud SQL Instance\"\n",
    "    assert pg_password, \"‚ö†Ô∏è Please provide the Password for the PG_USER\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíæ **Save Configuration to File** \n",
    "Save the configurations set in this notebook to  `config.ini`. The parameters from this file are used in notebooks and in various modeules in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import save_config\n",
    "\n",
    "save_config(embedding_model, vector_embedding_model, description_model, data_source, vector_store, logging, kgq_examples, PROJECT_ID,\n",
    "            pg_region, pg_instance, pg_database, pg_user, pg_password, pg_schema, \n",
    "            bq_dataset_region, bq_dataset_name, \n",
    "            bq_opendataqna_dataset_name, bq_log_table_name, bq_table_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è **Database Setup for Vector Store: CloudSQL-pgvector**\n",
    "\n",
    "Create PostgreSQL Instance on CloudSQL if 'cloudsql-pgvector' is chosen as vector store\n",
    "\n",
    "Note that a PostgreSQL Instance on CloudSQL already exists if 'cloudsql-pg' is the data source. PostgreSQL Instance is created only if a different data store is chosen.\n",
    "\n",
    "The cell will also create a dataset to store the log table on Big Query, **if** logging is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_setup import create_vector_store\n",
    "# Setup vector store for embeddings\n",
    "create_vector_store()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "opendataqna-venv",
   "name": "workbench-notebooks.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m122"
  },
  "kernelspec": {
   "display_name": "opendataqna-venv (Local)",
   "language": "python",
   "name": "opendataqna-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
